# Lecture 5 script: Databases and SQL for Social Data

## Slide 1 — Title

In this lecture we’re going to talk about databases and SQL for social data. The goal is not to turn you into a database engineer. The goal is to make you comfortable thinking relationally—because relational thinking is the foundation for most scalable social data workflows.

If you’ve ever had a project where you started with a CSV and then you ended up with five CSVs and a bunch of joins in memory, you’ve already run into the core problem: as projects scale, flat files become fragile, joins become expensive, and reproducibility suffers.

Databases solve that by giving you structure, integrity, and performance—and by letting you express data construction as a set of explicit, versionable queries.

**[pause]**

---

## Slide 2 — Why databases matter

This slide is the motivation.

As data scale increases, flat files become fragile. What do I mean by fragile? It becomes easy to accidentally overwrite things, to lose track of which file is “the final version,” to break joins because one file has different identifiers, or to generate duplicates that you don’t notice until much later.

Joins also become expensive. Even with modern laptops, if you’re joining large tables in memory repeatedly, you start to wait a long time, or you run out of RAM, or you do workarounds that make the pipeline harder to reproduce.

And reproducibility suffers because the transformations often happen in ad hoc ways—manual exports, temporary merges, intermediate files that aren’t documented.

Databases provide structure, integrity, and performance: structure through explicit schemas, integrity through keys and constraints, and performance through indexing and query optimization.

So the big idea is: databases reduce the number of ways your pipeline can quietly go wrong.

---

## Slide 3 — Thinking relationally

A database organizes data as tables, rows, and columns. That part is familiar.

What’s less familiar is the key line: meaning emerges from relationships across tables.

Relational thinking means you stop imagining “the dataset” as one table. Instead, you imagine the world as multiple entity types and multiple relationships.

For example: suppose you’re studying conflict events. You might have an events table, an actors table, a countries table, a locations table, and then relationships between them.

Relational thinking is essentially: define entities, define identifiers, define relationships, and then use joins to assemble the analytic view you need.

And that’s a methodological point: you are deciding what entities exist and what the unit of analysis is by how you design the schema.

**[pause]**

---

## Slide 4 — Tables are not datasets

A table represents one entity type and one level of analysis.

That’s the main trap new researchers fall into: they try to force everything into one giant table, and then they spend the rest of the project cleaning a mess.

In relational workflows, complex questions require multiple tables. A clean schema is often the difference between a pipeline that is stable and one that is constantly breaking.

So when you look at a research question, you should ask: what are the natural entities? What is an observation? What are the identifiers? And which relationships will I need to construct?

If those questions are answered in the schema, everything downstream becomes simpler.

---

## Slide 5 — Keys and identifiers

Databases rely on primary keys, foreign keys, and stable identifiers.

A primary key uniquely identifies a row in a table. A foreign key is a reference to a primary key in another table. And stable identifiers mean identifiers that persist across time and across processing steps.

The key conceptual line here is: keys encode assumptions about identity.

If you choose “country_name” as a key, you are assuming country names are stable and uniquely coded. Often they are not. If you choose numeric codes, you are assuming the coding scheme is consistent across sources. If you choose an “actor_id,” you are assuming you have a defensible definition of what counts as the same actor across time.

So keys are not just technical requirements. They represent the identity decisions underlying the dataset.

---

## Slide 6 — Schemas as measurement

A schema specifies table structure, variable types, and allowed values.

And the point is: schema design is a measurement decision.

Why? Because types and constraints determine what can enter the dataset. If you specify a field as an integer, you prevent certain malformed values from entering. If you specify allowed categories, you prevent uncontrolled category drift. If you specify uniqueness constraints, you prevent duplicate records.

You can think of a schema as a formalization of measurement rules.

In a lot of social science, the schema is implicit—hidden in code and assumptions. Databases force you to make it explicit.

**[pause]**

---

## Slide 7 — SQL as a query language

SQL expresses what data you want, how tables relate, and how results are filtered and aggregated.

And then the engine handles optimization.

This is one of the practical advantages: SQL is declarative. You specify the output you want, and the database decides how to execute it efficiently.

So instead of writing nested loops in Python or R to join, filter, and aggregate, you can express the transformation as a query—and the engine can use indexes and query planning to do it faster and more reliably.

A second advantage is reproducibility: queries can be versioned. You can store them as files. You can run them again. They become part of the pipeline artifact.

---

## Slide 8 — Basic SQL pattern

Here’s the simplest pattern: select from a table with a filter.

The key line is “declarative, not procedural.”

In a procedural mindset, you think: “Load the data, then loop through rows, then if year equals 2020 keep it.”

In a declarative mindset, you think: “I want all rows where year equals 2020.”

It’s a subtle difference, but it matters for scale and clarity. Declarative queries are often easier to audit: the logic is directly visible in the query.

---

## Slide 9 — Joins encode theory

This is the most important slide in the deck.

Joins combine tables via keys. Which join you choose changes results.

This is not just a technical warning. It’s a methodological warning.

A join is a claim about correspondence: you’re claiming these two tables refer to the same unit via this key. If the key is imperfect, you get mismatch. If the relationship is one-to-many or many-to-many and you assume it’s one-to-one, you can silently duplicate rows.

So the join is not “just merging.” It’s a theory of identity and relationship.

A really good habit here is: after you join, check row counts, check uniqueness, and verify that the join behaved the way you think.

---

## Slide 10 — Indexes and performance

Indexes speed up queries but increase write cost and reflect expected access patterns.

So indexing is a tradeoff: you make reads faster at the cost of extra storage and slower writes.

In social science workflows, the key intuition is: if you are repeatedly filtering or joining on a field—like a country code, a date, or an ID—then indexing that field can make your pipeline dramatically faster.

Indexes also reflect your expectations: what fields will be used to retrieve data? What joins will be common? What aggregations will you do?

So you can think of indexes as a performance model of your research workflow.

---

## Slide 11 — Embedded databases

For research workflows, embedded databases like SQLite and DuckDB are often the sweet spot.

They require no server. They integrate cleanly with scripts. And they scale well enough for many research projects.

This matters because a lot of people think “database” means you need to run a server and manage credentials. For many research pipelines, you don’t. A file-based database gives you most of the benefits with low overhead.

So the course emphasis is: use the simplest database technology that gives you structure, integrity, and speed.

---

## Slide 12 — Querying with DuckDB (Python)

This is an example of the simplest workflow: connect to a DuckDB file and execute a SQL query from Python.

The conceptual point is the last line on the slide: you’re calling SQL from analysis code. That’s the pattern we want.

Rather than exporting a CSV and then analyzing it, you can store the data in the database and retrieve just what you need, when you need it, using versioned queries.

That makes analysis more reproducible and more scalable.

**[optional shortener]** If you want to shorten: “connect, execute SQL, fetch results into a dataframe.”

---

## Slide 13 — Querying with DuckDB (R)

Same pattern in R: connect through DBI, run a query, retrieve results.

Again, the point is language independence: the database is the data layer; Python or R is the analysis layer. SQL is the interface between them.

That separation helps with reproducibility. It also helps collaboration: people can share the same data layer even if they use different analysis tools.

---

## Slide 14 — SQLite as an alternative

SQLite is file-based, widely supported, and works well for moderate-sized data.

Often it’s sufficient.

A useful mental model: if your project is not so large that it needs distributed computing, SQLite may be a perfectly good option. DuckDB tends to excel at analytics workloads; SQLite is ubiquitous and reliable. Both are good teaching choices.

The important point is: the technology choice should be guided by pipeline needs, not hype.

---

## Slide 15 — Querying from analysis code

Best practice: keep SQL in strings or files, call from Python or R, avoid manual exports. Queries are part of the pipeline.

This is a critical reproducibility lesson.

Manual exports create invisible states: “the version of the CSV I happened to export last Tuesday.” Queries in code create explicit states: the output is exactly what the query defines.

If a result changes, you can look at version history. If you want to rerun, you rerun. If a collaborator wants to reproduce, they run the same queries.

So a good pipeline treats queries like functions—inputs, outputs, and documented assumptions.

---

## Slide 16 — Databases and reproducibility

Databases support explicit schemas, versioned queries, consistent joins, and auditable transformations.

This reduces hidden data manipulation.

This slide is really a bridge between this week and the later reproducibility week. Reproducibility is not only about Git and containers. It’s also about eliminating ad hoc transformations and making data construction explicit.

If you design a schema and build data through queries, your pipeline becomes much easier to audit.

And note: “auditable transformations” is a research value. It lets you defend your results and identify where mistakes could have entered.

---

## Slide 17 — Common failure modes

This slide is basically a checklist of how database workflows go wrong.

Implicit joins are dangerous—joining without specifying keys or relying on defaults.

Duplicated rows can happen when the join relationship isn’t what you think.

Missing keys lead to dropped matches or NULLs that propagate.

And silent many-to-many merges are one of the most common and severe errors: you join two tables on a key that is not unique in either table, and suddenly your row count multiplies. The code runs fine. The results look plausible. But your dataset has been duplicated.

So the habit I want you to build is: always inspect join results. Check counts. Check uniqueness. Verify that the merged dataset matches your expectations.

---

## Slide 18 — What we emphasize in practice

Let me summarize the course habits for this week.

Design schemas deliberately. This is the upfront work that saves you later.

Use SQL for data construction. Let the database do what it’s good at.

Query from scripts, not GUIs. Keep the pipeline reproducible.

Treat joins as theoretical choices. Joins are where identity assumptions live.

If you do those four things, you’ll build data layers that are stable, scalable, and defensible.

---

## Slide 19 — Discussion

Let me close with the discussion questions.

What entities matter in your data? If you had to design a schema today, what are the key tables?

Where do joins encode assumptions? What keys would you trust, and what keys would you worry about?

And when does SQL outperform flat files? In general: when joins and filtering are central, when data are large, when you care about repeatability, and when you need an auditable record of transformations.

**[pause]**


