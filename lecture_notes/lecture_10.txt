# Lecture 10 script: Reproducibility, Git, and Research Artifacts

## Slide 1 — Title

In this lecture we’re going to focus on reproducibility—specifically, reproducibility as a research practice and as a design constraint.

Up to this point in the course, we’ve been building pipelines: web data collection, APIs, survey data processing, databases, record linkage, text pipelines, networks, and spatial workflows. A common thread is that these pipelines involve many decisions and many moving parts.

Reproducibility is the discipline that makes those decisions visible and those moving parts runnable.

So today I’ll cover three layers:

1. what reproducibility is and how it relates to the replication crisis,
2. why multiple comparisons and specification search matter even when you’re using strong designs like natural experiments,
3. and then the practical tooling: environment capture, Git workflows, containers, and what it means to produce a research artifact.

**[pause]**

---

## Slide 2 — Why reproducibility matters

Reproducibility asks a simple question: can someone else obtain the same results using the same data and code without informal guidance?

That last phrase—without informal guidance—is the key. If your project “works” only if you sit next to someone and explain which files to run, which settings to flip, and which versions to install, then it’s not reproducible.

And reproducibility is a minimum standard for credible research. It doesn’t guarantee the claim is true. But it guarantees the claim is inspectable.

In this course, I want you to treat reproducibility not as a polishing step at the end, but as a design choice from the beginning. If you build reproducibility in early, everything becomes easier: collaboration, debugging, validation, and future reuse.

---

## Slide 3 — The replication crisis

Now let’s connect this to the replication crisis.

Across the social and behavioral sciences, many published findings fail to replicate; effect sizes shrink or disappear; and results depend on analytic choices.

And the key line is: this is not about fraud; it is about systems.

What does that mean? It means that the incentives, workflows, and publication norms can produce a world where fragile findings survive. It’s not that most researchers are dishonest. It’s that researchers have degrees of freedom, journals select for novelty, and many results are underpowered or sensitive to choices.

So reproducibility is part of the response because it makes analytic choices visible and results checkable.

**[pause]**

---

## Slide 4 — Sources of replication failure

This slide lists common contributors: researcher degrees of freedom, small samples and noisy estimates, selective reporting, and multiple hypothesis testing.

A useful way to think about this is: many failures are not “errors,” they’re outcomes of flexibility.

If a researcher can reasonably choose among many modeling decisions, then some specifications will produce “significant” results by chance. If only those results are reported, the published record becomes biased toward false positives.

And that’s why replication efforts often find smaller effects: the original literature contains a selection mechanism.

So the big lesson here is: good research practice is not only about better models. It’s also about constraining flexibility, making choices transparent, and evaluating robustness honestly.

---

## Slide 5 — Multiple comparisons in practice

Multiple comparisons matter because many studies implicitly test multiple outcomes, multiple specifications, and multiple subgroups.

Even with a single natural experiment, false positives accumulate without correction.

Let me make that intuitive. Suppose you have a policy shock and you test ten outcomes. Even if the policy has no true effect on any outcome, you should expect some low p-values just by chance.

Or suppose you estimate the same effect under a range of control sets and windows. Some combinations will produce “significance” by luck.

So multiplicity is not only an issue in exploratory work. It is a structural issue in empirical practice.

The course takeaway is: if your pipeline implicitly encourages many tests, you need a plan for how you will interpret them—adjustments, pre-specification, or at minimum transparent reporting.

---

## Slide 6 — Natural experiments and multiplicity

This slide is specifically about why multiplicity shows up even when you think your design is strong.

Natural experiments often involve many outcomes per treatment, event-study coefficients over time, and sensitivity across windows and controls.

Event studies are a great example. An event study is not one estimate. It’s a path of estimates: multiple coefficients, each a separate hypothesis. Looking at that path and visually interpreting it without adjustment can lead to overconfidence.

Another example is bandwidth sensitivity in RDD: different bandwidths yield different estimates, and people may choose the one that “looks right” or “is significant.”

So multiplicity is structural, not accidental. It comes from the fact that robust inference often requires checking many angles—and the moment you check many angles, you have many potential false positives.

So the lesson is not “never check robustness.” The lesson is to recognize that robustness checks themselves can become a search process if you don’t discipline them.

---

## Slide 7 — Specification search

This slide is the risk scenario: trying alternative controls, varying bandwidths or cutoffs, exploring subgroups post hoc.

Without discipline, robustness becomes overfitting.

Overfitting here doesn’t mean machine learning overfitting. It means you are fitting your narrative to the combination of choices that yields the most compelling result.

This is one reason why transparent workflows matter. If the full specification path is visible—what was tried, what was discarded—then readers can assess whether the result is robust or selected.

This also motivates tools like preregistration, multiverse analysis, and systematic robustness reporting. You don’t have to adopt all of those in every project, but you should understand why the problem exists.

---

## Slide 8 — What replication really tests

Replication evaluates data access, code executability, analytic transparency, and sensitivity to assumptions.

Exact numerical equality is rarely the point.

That last line matters because in computational pipelines, exact equality is often too strict. Floating-point differences, parallelization, random seeds, and version drift can produce slightly different numbers.

The deeper point is whether the qualitative conclusions hold and whether the analytic pipeline can be rerun and inspected.

So when we talk about replication, it’s helpful to separate:

* **computational reproducibility**: can I run your code and get your outputs?
* **replication of results**: do we get the same substantive conclusion when we implement the same method?
* **robustness / generalization**: does the conclusion hold under reasonable variations or new samples?

This lecture focuses most on computational reproducibility and research artifacts, because those are prerequisites for the other two.

---

## Slide 9 — Computational reproducibility

Even with shared code and data, results may differ across machines, library versions change behavior, and randomness is handled differently.

This is one of the most underestimated problems in graduate training. People assume “code is deterministic.” In practice, it often isn’t.

Randomness: if you don’t fix seeds, models can differ across runs. Even with seeds, some libraries use nondeterministic kernels.

Library behavior: default settings change over time. A function that used to sort one way may sort another way. A model fitting function may change an optimizer.

So computation is part of the research design. If your pipeline depends on defaults you didn’t specify, then your pipeline is fragile.

---

## Slide 10 — Operating system differences

Results can vary across Linux, macOS, and Windows: file system behavior, floating-point implementations, and path handling.

This is where practical collaboration pain comes from. A script that uses Windows-style file paths breaks on Linux. A file name that differs only in capitalization can behave differently across file systems. Line endings can differ. Some compiled dependencies behave differently.

So assuming identical environments is unsafe.

The takeaway is: if you want work to be reproducible across machines, you must treat the execution environment as part of the project. That includes path conventions, dependency management, and explicit instructions.

---

## Slide 11 — Version drift

Over time, R and Python packages update, default behaviors change, and deprecated functions break pipelines.

Code that ran once may not run again.

This is why “I put my code on GitHub” is not the same as “my results are reproducible.”

If you don’t capture versions, your collaborator might install a newer package version and get a different result, or the script may fail entirely.

So the habit is: treat dependency versions as part of your project.

---

## Slide 12 — Environment capture

Best practice includes explicit package versions, lockfiles—like renv in R or requirements.txt or a lockfile in Python—and minimal system dependencies.

Environments must be specified, not inferred.

This is a key idea: don’t rely on the reader to guess your environment. Make it explicit and easy to recreate.

In practical course terms, this is why we care about:

* `requirements.txt` or a lockfile in Python,
* `renv.lock` in R,
* and potentially container definitions.

The goal is not perfection. The goal is that someone can run your pipeline without heroic troubleshooting.

---

## Slide 13 — Git as a scientific tool

Git supports versioned code history, branching and experimentation, and transparent collaboration.

Git records how results evolved.

This is a major shift from “files on my laptop.” Git creates a history of changes. That history is part of scientific transparency.

It also supports collaboration: you can propose changes via branches and pull requests, review them, and merge them in a controlled way.

So in this course, we treat Git as part of scientific instrumentation, not as a bureaucratic hurdle.

---

## Slide 14 — Commits as research units

Each commit should represent a logical change, be reversible, and be interpretable.

Commit history is a research log.

This is a practical way to think about discipline. A clean commit history makes your project easier to debug, easier to audit, and easier to explain.

A useful heuristic: a commit message should tell you what changed and why. Not in a verbose way—just enough to understand the intent.

In the same way a lab notebook records experiments, your commit history records analytic evolution.

---

## Slide 15 — Containers

Containers bundle code and dependencies, isolate execution environments, and improve portability.

They reduce, but do not eliminate, variability.

The key point is “reduce.” Containers help because they standardize the environment—operating system libraries, dependency versions, and system packages.

But containers don’t solve everything. They don’t guarantee identical hardware, and they don’t solve conceptual errors.

Still, for reproducible pipelines, containers are often the closest thing to “run anywhere.”

---

## Slide 16 — Containers are not magic

Containers do not fix data errors, conceptual mistakes, or poor documentation.

They address execution, not inference.

This is a crucial clarification. If your data are biased, a container reproduces the bias. If your model is misspecified, a container reproduces the misspecification. If your README is unclear, a container doesn’t explain what to run.

So containers are a tool, not a substitute for method.

---

## Slide 17 — Research artifacts

A reproducible project produces data, code, documentation, and execution instructions.

The artifact is the contribution.

This is a core course value. If you build a high-quality data product—cleanly collected, well documented, validated, with a pipeline that others can run—then the artifact itself is a scholarly contribution.

And increasingly, that is how serious research is evaluated: not only by results, but by the quality and reuse potential of the underlying data and code.

---

## Slide 18 — Documentation is essential

Reproducibility requires clear READMEs, data provenance, known limitations, and explicit assumptions.

Undocumented pipelines are irreproducible.

This is the “soft” part that is actually hard. Documentation is the bridge between your mental model and someone else’s ability to run your work.

A good README answers:

* what the data are,
* how to obtain or access them,
* how to run the pipeline end-to-end,
* what outputs are produced,
* and what limitations are known.

In this course, we treat documentation as part of the pipeline, not as optional polish.

---

## Slide 19 — What we emphasize in practice

Let me summarize the four takeaways.

Treat reproducibility as design: build it in early.

Anticipate multiplicity: recognize where multiple comparisons and specification search arise, especially in natural experiment settings.

Capture environments early: lock versions, minimize dependencies, make reruns possible.

Make artifacts usable by others: the goal is that someone else can run your project with minimal friction.

If you do these four things, you will produce work that is easier to collaborate on, easier to validate, and much easier to defend.

---

## Slide 20 — Discussion

Let me close with three questions.

What kinds of results are hardest to reproduce? Is it models with nondeterminism, pipelines with many dependencies, or results that depend on specific versions?

Where does multiplicity enter your work? Is it multiple outcomes, multiple specifications, or repeated robustness checks?

And what is the minimal reproducible artifact? If you had to hand your project to someone else, what would they need to rerun it and trust it?

**[pause]**
