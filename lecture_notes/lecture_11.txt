# Lecture 11 script: Data Quality, Labeling, and Validation

## Slide 1 — Title

In this lecture we focus on data quality, labeling, and validation.

The big idea is simple: data quality determines what can be learned, which populations are visible, and how uncertainty propagates. And poor data quality cannot be fixed downstream.

That last sentence is worth taking seriously. You can run the best model in the world on low-quality data and still get fragile or biased conclusions. The modern challenge in big social data isn’t only analysis—it’s building data products that are trustworthy.

So today I’ll organize the lecture around three themes:

1. how bias enters during collection,
2. why labeling introduces additional uncertainty,
3. and what validation looks like when you can’t manually inspect everything.

**[pause]**

---

## Slide 2 — Why data quality is central

Data quality determines what can be learned, which populations are visible, and how uncertainty propagates.

This means data quality is not a technical footnote. It is a central part of scientific inference.

A useful way to frame it: data quality constrains the space of plausible conclusions. If certain populations are missing, you can’t generalize to them. If measurement error is systematic, effect estimates can be biased, not just noisy. If errors are correlated with covariates, you can induce spurious relationships.

So we treat data quality as a design problem, not a cleaning problem.

---

## Slide 3 — Validity threats in big social data

Here are four common validity threats we’ll return to repeatedly: coverage bias, measurement error, selection effects, and temporal instability.

Coverage bias is about who or what is visible in the data at all.

Measurement error is about whether what’s recorded corresponds to the concept you care about.

Selection effects are about how inclusion depends on behavior or outcomes—for example, opt-in samples.

Temporal instability is about drift—data-generating processes changing over time.

And the key line matters: scale amplifies rather than removes these problems.

It’s easy to assume that “more data” solves validity issues. Often it doesn’t. If you have a million observations but they all come from the same biased process, you get very precise estimates of the wrong thing.

So big data can make you confidently wrong.

---

## Slide 4 — Bias enters at collection

Bias enters at collection because collection processes shape who is observed, what is recorded, and which events are missing.

And importantly: bias is often structural, not accidental.

Structural bias means it is baked into the data-generating process: institutions and platforms systematically produce some kinds of records and not others. It’s not random “noise.” It is predictable and patterned.

So the habit is to ask, for any dataset: what is the process that produces records, and what kinds of records does that process systematically fail to produce?

---

## Slide 5 — Media-sourced event data

Event data based on news reporting reflect journalist locations, editorial priorities, press freedom, and resources.

So events in well-covered areas are overrepresented.

This is a foundational point for a lot of political science data. If you build a conflict event dataset from news, you are building a dataset about “reported events,” not necessarily events.

The reporting process becomes part of the measurement. Reporting is shaped by where journalists are, what editors prioritize, what audiences demand, and what can be safely reported.

So, for example, urban areas tend to be covered more than rural ones. High-profile actors tend to be covered more than marginal groups. Countries with press restrictions may be underreported or selectively reported.

So the dataset reflects a joint process: events plus the reporting mechanism.

---

## Slide 6 — Geographic reporting bias

The consequences include urban bias, underreporting in rural or conflict zones, and systematic cross-national differences.

And the line on the slide is the key: absence of reports does not imply absence of events.

This is where analysts often get tripped up. They interpret zeros as “no events,” when the zero might mean “no recorded events,” and that might mean “no journalists,” “no connectivity,” “no press freedom,” or simply “no attention.”

So one validation habit is to check whether the “zeros” correlate with known reporting capacity measures—population density, proximity to major cities, press freedom, or infrastructure.

Another habit is triangulation: comparing event data to alternative sources when possible.

---

## Slide 7 — Social media as data

Social media data represent platform-specific user populations, non-random participation, and algorithmic amplification.

Users are not representative of populations.

This is another version of the same theme: you don’t have “public opinion.” You have “public opinion among people who use platform X, who choose to post, and whose posts are surfaced by platform algorithms.”

That doesn’t mean social media is useless. It means you must be explicit about what population is represented.

Coverage bias: who is on the platform?

Selection bias: who posts, and what do they post?

Algorithmic distortion: what gets amplified and what gets suppressed?

So social media data are a joint product of user behavior and platform governance.

---

## Slide 8 — Opt-in and platform studies

Opt-in data involve self-selection, differential attrition, and strategic participation.

Inference requires strong assumptions.

This slide is important because it generalizes beyond social media. Many modern datasets are opt-in: people choose to participate, choose to remain, choose to provide information.

Self-selection means participants differ from non-participants.

Differential attrition means some groups drop out more than others.

Strategic participation means people may respond in ways that maximize incentives, misrepresent identity, or coordinate.

So when you analyze opt-in data, you need a clear story about what assumptions justify your inference—and you should be transparent about where those assumptions might fail.

---

## Slide 9 — Satellite and remote sensing bias

Remote sensing data face cloud cover and atmospheric interference, sensor resolution limits, and strategic behavior to avoid detection.

Measurement error varies across space and time.

This is an important reminder: “objective” sensors are not bias-free.

Cloud cover creates systematic missingness—some places and seasons are harder to observe.

Resolution limits mean you may only detect large changes, not small ones.

And strategic behavior matters: if actors know they are monitored, they may adapt behavior to avoid detection—timing, location, or scale.

So remote sensing changes the observation problem, but it doesn’t remove it.

A good habit is to ask: what are the conditions under which the sensor fails, and are those conditions correlated with the outcome?

---

## Slide 10 — Labeling and annotation

Now we move from collection to labeling.

Labels are often produced by human coders, crowdsourcing platforms, or machine-assisted workflows.

And labeling introduces additional uncertainty.

Even if the raw data were perfect, labels can be ambiguous. Categories can be unclear. Coders can interpret texts differently. Crowdsourcing can introduce effort variation. Machine assistance can introduce systematic hallucination or drift.

So labeling is not a final “ground truth” step. It’s another measurement procedure—one that should be evaluated.

---

## Slide 11 — Inter-coder reliability

Reliability assesses agreement across coders, consistency of interpretation, and ambiguity in categories.

High agreement does not guarantee validity.

This is a key conceptual point. Reliability is about consistency. Validity is about correctness relative to a concept.

You can have high reliability if coders consistently apply the same flawed rule. You can have low reliability because the concept is genuinely ambiguous or because the coding scheme is underspecified.

So reliability metrics are useful, but they are not enough. They tell you something about the coding process, not necessarily about whether the measure captures the construct you care about.

---

## Slide 12 — Reliability example (conceptual)

This slide is just a reminder that there are formal agreement statistics—like Cohen’s kappa—and that you can compute agreement across annotators.

The bigger message is: treat labeling as something you can evaluate quantitatively.

You can do:

* spot checks,
* double coding on subsets,
* adjudication protocols,
* and agreement statistics.

And you should design labeling so that evaluation is possible.

---

## Slide 13 — Validation beyond labels

Validation includes spot checks, external benchmarks, temporal consistency, and sensitivity analyses.

No single validation step is sufficient.

This is another course-wide theme: validation is layered. You do not “validate once.”

Spot checks tell you whether outputs look plausible.

External benchmarks tell you whether measures align with known facts or alternative data sources.

Temporal consistency checks whether the measure behaves reasonably over time.

Sensitivity analyses check whether results depend strongly on reasonable alternative choices.

So validation is an ongoing process that spans the pipeline—not just a postprocessing step.

---

## Slide 14 — Monitoring and drift

Over time, data-generating processes change, platforms update rules, and user behavior shifts.

Static validation is inadequate.

This is especially important in big social data because many sources are dynamic. A platform changes an algorithm. A government changes a reporting rule. A news outlet changes editorial priorities. A sensor changes calibration.

If you validate once, you might validate an old regime and then continue collecting data in a new regime without noticing.

So “monitoring” here means building checks that run repeatedly: distribution checks, missingness checks, metadata checks. You’re not just validating a dataset; you’re validating a pipeline that continues to produce data.

---

## Slide 15 — Documenting limitations

Good practice requires explicit statements of bias, known blind spots, and assumptions required for inference.

Transparency strengthens credibility.

This is where a lot of research falls short because it feels uncomfortable to list limitations. But in reality, limitations are always present. The question is whether you disclose them or hide them.

A credible data product is one where a reader can understand:

* what the data cover well,
* what they cover poorly,
* what might be systematically missing,
* and what assumptions are required to interpret results.

So documentation is not a weakness. It’s part of defensible inference.

---

## Slide 16 — Data quality as design

Quality should be anticipated at collection, monitored over time, and reported alongside results.

Validation is part of methodology.

This is the synthesis slide.

If you wait until the end to think about quality, you will discover problems when they are expensive or impossible to fix.

If you anticipate quality at collection, you can build in redundancy, auditing, and provenance.

If you monitor over time, you can detect drift.

And if you report alongside results, you prevent readers from overinterpreting.

So data quality is design. It belongs at the beginning of a project, not just the end.

---

## Slide 17 — What we emphasize in practice

Treat bias as structural. Don’t assume missingness is random.

Validate at multiple stages. Collection, cleaning, labeling, outputs.

Separate reliability from validity. Consistency is not correctness.

Document uncertainty explicitly. If the measure is fragile or biased, say so and show sensitivity when possible.

Those four principles are the practical ethics of big social data.

---

## Slide 18 — Discussion

Let me close with the discussion questions.

Which biases are hardest to detect? Often the hardest are the ones that look like “zeros” or missingness—because absence is easy to misinterpret.

When is validation infeasible? Sometimes you can’t get external benchmarks, or the ground truth doesn’t exist. In those cases, you rely more on sensitivity, audits, and transparent assumptions.

And how should uncertainty be communicated? Should you provide confidence intervals, sensitivity ranges, robustness checks, or qualitative caveats? What is sufficient depends on the stakes and the audience, but the core principle is: communicate uncertainty in a way that prevents overconfidence.

**[pause]**