# Lecture 3 script: APIs and Data Collection at Scale

## Slide 1 — Title

In this lecture we move from scraping web pages to collecting data through APIs—application programming interfaces.

The key theme is that APIs are not just technical endpoints. They are institutional and technical contracts. They structure what is visible, what is accessible, and what is feasible to collect at scale.

So today I want to walk through the conceptual logic of API-based data collection: what APIs are, what JSON is and why it matters, and then the practical patterns that make data collection reliable—authentication, pagination, rate limits, retries, logging, and storing raw responses.

**[pause]**

---

## Slide 2 — Why APIs matter for social data

Why do APIs matter for social data?

Many large-scale social datasets are accessed through APIs. They provide structured endpoints and machine-readable responses. And importantly, for many systems, an API is the intended interface for data access.

This is an important contrast with scraping. Scraping often means extracting data from a system that wasn’t designed to publish data. APIs usually mean the system is explicitly offering data access in a structured way.

But the existence of structure doesn’t eliminate judgment. It changes what judgment looks like. Instead of reverse-engineering HTML, you interpret endpoints, parameters, and response schemas—and those choices still affect measurement.

---

## Slide 3 — APIs are contracts

This slide is the conceptual anchor: APIs are contracts.

An API specifies what data can be requested, how requests must be structured, and what constraints apply to usage.

That includes explicit constraints like rate limits, authentication requirements, and quotas. But it also includes implicit constraints: the categories the API uses, the fields it returns, and what it does not return.

So when we use APIs for social science, we need to treat them as artifacts of institutional choices. Someone designed the endpoints, decided what counts as a record, decided which filters exist, and decided what metadata to expose.

All of that shapes what you can observe.

**[pause]**

---

## Slide 4 — APIs vs web scraping

Here are the key differences.

APIs expose structure explicitly. Scraping recovers structure from presentation.

APIs enforce access rules—through keys, tokens, and quotas. Scraping is often less formal but can be blocked in other ways.

APIs log and monitor usage. That matters because it means your access can change, your queries can trigger throttling, and the provider may have policies about acceptable use.

So: scraping is often about parsing. APIs are often about governance and constraints.

---

## Slide 5 — Common API response formats

Most social-science APIs return JSON objects: nested key–value structures and lists of records.

The key point is on the slide: understanding structure matters more than syntax.

When students first learn APIs, they often focus on the mechanics of making a request. But the core work is understanding the schema: what fields exist, what types they are, how nested objects are structured, and which parts of the response correspond to units of analysis.

If you treat an API response like a flat table right away, you’ll often make mistakes—like dropping nested metadata, mishandling missingness, or inadvertently duplicating units.

So the right mindset is: first, inspect the structure. Then decide how to translate it into your analysis-ready format.

---

## Slide 6 — JSON as a data structure

JSON represents objects, arrays, and nested hierarchies.

That’s why the last line matters: APIs return trees, not tables.

A table is one representation you might create, but it’s not the native representation. The native representation is hierarchical.

So a lot of practical work in API collection is:

* preserving the raw tree,
* extracting the pieces you need,
* and documenting the transformation.

There’s a methodological analogy to earlier lectures: HTML is a tree, and scraping is tree traversal. JSON is also a tree, and API ingestion is also tree traversal—just in a cleaner, explicitly structured way.

---

## Slide 7 — Authentication patterns

APIs often require API keys, tokens, or OAuth flows.

And authentication governs access, rate limits, and attribution.

So authentication is not just a technical nuisance. It also affects:

* whether you can collect at all,
* how fast you can collect,
* whether the data are restricted by account type,
* and whether the provider can audit your usage.

When you’re designing an API-based research pipeline, you should treat authentication as part of the research design. It determines feasibility and constraints.

**[pause]**

---

## Slide 8 — API requests in Python

This slide shows the basic request pattern in Python.

You define a URL endpoint, define parameters, define headers—often including authorization—and then send the request.

Then you parse the response as JSON.

The core conceptual step is: the endpoint plus parameters define what you’re asking for. That is where your measurement choices start.

For example, if the API supports a “limit” parameter, you need to decide batch size. If it supports filters like language, geography, or date range, you need to decide what boundaries define your dataset.

And those boundaries are not neutral. They shape what counts as an observation in your study.

**[optional shortener]** If you want to shorten: “endpoint, params, headers, request, parse JSON.”

---

## Slide 9 — API requests in R

The R version is structurally identical: define URL, attach query parameters, attach headers, request, parse.

Again, the point is that the method is conceptual. The code differs, but the logic is the same.

In this course, what I care about is that you can look at a request and understand what it is doing, what assumptions it encodes, and what failure modes are likely.

---

## Slide 10 — Pagination is the norm

Large APIs rarely return everything at once. That’s not a bug; it’s a design choice.

Common pagination patterns include page numbers, offsets, cursor-based pagination, and time-based windows.

The methodological angle: pagination is part of measurement. How you paginate affects completeness and bias. If your pipeline drops pages when requests fail, you get missingness. If the API returns results in a particular order and you only collect the first N pages, you build truncation bias.

So pagination is not just “a loop.” It’s a key part of ensuring that you have actually collected what you think you collected.

---

## Slide 11 — Pagination loop in Python

Here’s a conceptual pattern: initialize an accumulator, iterate through pages, append results, and stop when you reach an empty batch.

A few things to notice.

First, this is where logging becomes crucial. If a single page fails, you need to know which page failed and whether you retried.

Second, this is where deduplication matters. Some APIs can return overlapping records between pages if the underlying data are changing. If you collect over time, you might see the same record appear again. So a robust pipeline deduplicates by a stable ID if one exists.

Third, this is where idempotence matters: you want to be able to resume from where you left off without corrupting the dataset.

Those are the scale themes we’ll keep repeating.

---

## Slide 12 — Pagination loop in R

Same pattern in R: repeat, request, extract batch, break if empty, otherwise accumulate and increment page.

The main lesson is not the syntax. It’s that your data collection pipeline has to be designed for repeated execution and partial failure.

---

## Slide 13 — Rate limits exist for a reason

Rate limits are enforced because APIs serve many users, and providers manage load, cost, and policy risk.

They can enforce requests per second, requests per day, and burst limits.

If you violate limits, you get errors, temporary bans, or revoked access.

Two key implications for research pipelines:

One: you need to design your pipeline to be “polite” by default—slow down, respect quotas, and keep your requests stable.

Two: rate limits shape feasibility. If you need millions of records but you can only request a small number per hour, your research design may need to change—either by collecting over a longer window, narrowing the query, or using alternative sources.

So rate limits are not just technical constraints. They shape research questions.

---

## Slide 14 — Respecting rate limits

The good practice list here is straightforward: sleep between requests, check headers, and back off on errors.

That’s the beginning of responsible collection.

But I want to emphasize something: “backing off on errors” is also part of measurement. If you get an error response and simply skip it, you introduce missingness that is correlated with system load or with request patterns. That can bias your dataset.

So a robust pipeline handles errors intentionally: it retries, logs failures, and ideally records which queries were not collected successfully.

---

## Slide 15 — Retries and failures

At scale, failures are normal: network errors, timeouts, server-side issues.

So your code should expect failure.

This is a mindset difference between toy scripts and research pipelines. In toy scripts, an error is a surprise. In pipelines, an error is expected, and the question is whether you handle it in a controlled way.

We’ll treat reliability as part of your method: not because reliability is glamorous, but because unreliable collection produces untrustworthy data.

---

## Slide 16 — Retry pattern

This is a conceptual retry pattern: try up to a fixed number of attempts, raise errors when appropriate, and wait between attempts.

A few practical notes.

First, you should choose retry logic based on error types. Some errors are permanent—like authentication failure. Others are transient—like timeouts. Treating them the same can produce either wasted time or silent missingness.

Second, retries should be logged. If you’re getting repeated failures on a particular parameter combination, that can indicate a structural problem—like endpoint instability or rate limit enforcement.

Third, retries should not become infinite loops. A pipeline that never stops is not robust; it’s opaque.

So, retries are about balancing persistence with transparency.

---

## Slide 17 — Logging as data collection

Logging is not an add-on. Logging is part of data collection.

Logs help answer: what was collected, when, what failed, and what was skipped.

And that matters for two reasons.

One: you need logs to debug. Without logs, you don’t know whether your pipeline failed early, failed intermittently, or failed systematically.

Two: you need logs for provenance. If you want someone else to trust your dataset, you need to be able to say how it was constructed, and what the known gaps are.

So when we build API workflows, we’ll treat logs as an output artifact, not an internal detail.

---

## Slide 18 — Storing raw API responses

This slide is one of the most important practical habits: store raw responses.

Best practice is to save raw JSON, not overwrite, keep timestamps, and separate raw from processed data.

Raw data are the audit trail. They let you reconstruct what you saw at the time of collection. They let you rerun parsing logic later without re-querying the API. And they let you detect whether parsing decisions changed outcomes.

In other words: the raw layer is how you protect yourself against drift, mistakes, and later questions about data integrity.

---

## Slide 19 — Data provenance matters

Every API workflow should document the endpoint used, parameters, authentication method, collection date, and known limitations.

APIs change over time. Fields get renamed. Default behaviors change. Quotas change. Even the interpretation of fields can drift.

So provenance is not just “nice to have.” It’s what makes an API dataset credible.

And note: provenance is also what allows you to compare datasets collected at different times. If you collect again next year, you need to know what changed.

---

## Slide 20 — What we emphasize in practice

Let me summarize.

Treat APIs as institutional artifacts. They encode choices about what is visible.

Inspect structure before modeling. Don’t flatten JSON blindly.

Expect scale to surface failure. Design for it: retries, logging, idempotence.

Preserve raw data and logs. That’s the backbone of reproducibility and auditability.

If you do those things, you’ll build pipelines that you can defend—and you’ll avoid the most common failure mode in API research, which is quietly collecting something different from what you thought you collected.

---

## Slide 21 — Discussion

Let me close with three questions.

What assumptions do APIs embed? Look at fields, filters, categories, and defaults.

Who controls access and visibility? Providers can change policies, enforce quotas, and define what is observable. That can shape research agendas in subtle ways.

And how do rate limits shape research questions? Sometimes rate limits force you to sample, narrow, or redesign the question. Those are scientific consequences of infrastructure.

