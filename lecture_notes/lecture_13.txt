# Lecture 12 script: LLM-Assisted Data Extraction (Human-in-the-Loop)

## Slide 1 — Title

In this lecture we’re going to cover LLM-assisted data extraction, with a very specific framing: human-in-the-loop.

The goal is not to treat an LLM as a magical automation tool. The goal is to use LLMs as scalable assistance for turning messy, unstructured text into structured data—while keeping scientific responsibility with the researcher.

So today I’ll walk through an extraction pipeline: defining schemas, designing prompts, running batch workflows, handling uncertainty, validating with human review, documenting failure modes, and evaluating performance.

The thread throughout is the same as earlier weeks: extraction is measurement, and measurement requires validation.

**[pause]**

---

## Slide 2 — Why use LLMs for data extraction

Many social-science data sources are messy, unstructured, text-heavy, and expensive to code by hand.

This is exactly the gap where LLMs can help.

Think about tasks like: extracting actors and actions from news reports, coding whether a statement contains a threat or a concession, identifying treaty provisions in legal text, coding the type of event described in a paragraph, or extracting structured fields from narrative reports.

Historically, you’d do these tasks with hand coding or with brittle rule-based systems. LLMs provide a new option: a flexible model that can map text to structured fields.

But the key line is the one on the slide: LLMs offer scalable assistance, not automation.

That phrase captures the course’s stance. You can use LLMs to accelerate work—but you must design the pipeline so that outputs are auditable and errors are visible.

---

## Slide 3 — Human-in-the-loop philosophy

Here’s the philosophy we’re using in this course.

LLMs are used to propose structured outputs, accelerate labeling, and surface uncertainty.

Humans remain responsible for validation.

This is crucial because the highest risk failure mode with LLMs is not that they fail loudly. It’s that they produce plausible outputs that are wrong, and the pipeline treats those outputs as ground truth.

So human-in-the-loop means: the model is a first pass, not a final authority. The pipeline is designed so that we can detect errors, audit performance, and intervene when uncertainty is high.

In practical terms, this often means:

* a confidence or uncertainty signal,
* thresholds that route uncertain cases to review,
* spot audits even when confidence is high,
* and systematic checks for failure modes.

---

## Slide 4 — Structured extraction

Structured extraction means defining a schema in advance, forcing outputs into fields, and rejecting free-form text.

Schemas discipline model behavior.

This is one of the most important design principles.

If you ask an LLM, “Summarize this document,” you get a summary. That’s not a dataset.

If you ask: “Extract an actor, an action, a target, a date, and a confidence score, and return valid JSON,” you’re building the foundation for a dataset.

The schema forces you to commit to what you want to measure. It also gives you a way to validate outputs—because you can check whether the output matches the schema.

So the key idea is: structure first. Then scale.

---

## Slide 5 — Schema example

Here’s an example schema: actor, action, target, date, confidence.

The important thing is not the specific fields. The important thing is that you have explicit types and explicit expectations.

The phrase “explicit schemas reduce ambiguity” is exactly right.

Ambiguity is where LLMs behave unpredictably. If “actor” could be an organization or a person, decide in advance. If “date” could be a range or a single day, decide in advance. If “confidence” is required, define what it means—model self-report versus a derived uncertainty score.

A good schema also encourages consistency across documents, which is essential if you want to aggregate results.

---

## Slide 6 — Prompt design for extraction

Effective prompts describe the task narrowly, specify output format, and include examples when possible.

Prompting is part of the method.

This is a key scientific point. Prompt design is not “UI.” It’s a specification of the measurement procedure.

If you change the prompt, you change what the model is asked to do. That can change outputs in systematic ways.

So in this course we treat prompts like code: version them, document them, and evaluate them.

A narrow task definition reduces hallucination. Clear output format reduces parsing failures. Examples can anchor the model to your coding rules.

---

## Slide 7 — Prompt example

This slide shows a simple pattern: instruct the model to extract fields and return JSON matching the schema, then provide the text.

The main point to stress is that the prompt contains three components:

1. task instruction,
2. output constraint,
3. input text.

In more mature pipelines, you might also include:

* definitions for each field,
* a “do not guess” instruction,
* an explicit “if missing, return null” rule,
* and one or two examples of correct output.

But even in this simple form, the structure is what matters: you are treating the LLM as a structured extractor, not as a chatbot.

---

## Slide 8 — Batch processing

LLMs are typically applied in batches, with rate limits and cost constraints.

Pipelines must manage scale.

This is where the “data engineering” side of LLMs shows up.

In practice, you need to design batch processing to:

* respect provider limits,
* avoid losing progress,
* handle partial failures,
* control costs,
* and produce outputs in a consistent form.

So the pipeline mindset from earlier weeks returns: idempotence, logging, retries, raw output storage.

The difference is that you now have a model in the loop, and model responses can vary.

---

## Slide 9 — Batch processing pattern

This slide gives the basic loop: for each document, call the LLM, append the result.

The key line is: batch size affects cost and reliability.

Batch size affects throughput and rate limiting. It also affects how you organize retries and how you checkpoint results.

A practical pipeline should never assume that all calls succeed. It should:

* save results incrementally,
* record which documents have been processed,
* and allow restarts without duplicating work.

If you take one lesson from this slide: a robust LLM extraction pipeline is more like an API collection pipeline than a one-off model call.

---

## Slide 10 — Uncertainty and confidence

LLMs can provide confidence scores, flag ambiguous cases, and abstain when uncertain.

Uncertainty should be captured explicitly.

This is where the course’s validation principles become operational.

If every output is treated as equally reliable, you have no way to prioritize human review. But if you capture uncertainty, you can route the most ambiguous cases to humans, and you can audit performance stratified by confidence.

One important caveat: model-reported confidence is not always calibrated. It can be helpful, but you should validate it. Overconfident errors are common.

So in practice, you often combine:

* self-reported confidence,
* heuristic signals like format violations or missing fields,
* and downstream calibration from audited samples.

---

## Slide 11 — Uncertainty example

This is the simplest routing rule: if confidence is below a threshold, flag for review.

The point isn’t the exact threshold—0.6 here is just illustrative.

The point is the workflow: the model produces an output plus a signal, and the pipeline uses that signal to decide whether a human needs to look.

Human-in-the-loop is not just “humans check a few things.” It’s designing an explicit decision rule for how human effort is allocated.

---

## Slide 12 — Human validation

Human review is used to spot-check outputs, correct systematic errors, and refine prompts and schemas.

Validation is iterative.

This is crucial. LLM pipelines improve through iteration.

You run an extraction pass. You audit outputs. You identify failure modes. You adjust the schema or prompt. You rerun.

This looks a lot like other forms of measurement development. It’s closer to building a survey instrument than running a model once.

So you should expect refinement cycles. The pipeline is not “prompt once and hope.” It is “design, test, audit, revise.”

---

## Slide 13 — Spot audits

Auditing strategies include random sampling, stratified checks, and edge-case review.

Audits reveal failure modes.

Random sampling tells you overall performance.

Stratified checks let you focus on segments where performance might differ—by document type, length, source, or predicted confidence.

Edge-case review is where you test adversarial or ambiguous examples—where you expect the model to struggle.

This is exactly how you turn LLM extraction from a demo into a method: you build an evaluation plan that makes failures visible.

---

## Slide 14 — Failure modes

Common failures include hallucinated fields, overconfident errors, inconsistent formatting, and sensitivity to phrasing.

Failures must be documented.

Let me emphasize the difference between these.

Hallucinated fields: the model invents content not present in the text.

Overconfident errors: the model gives a plausible but wrong extraction and expresses high confidence.

Inconsistent formatting: the model violates the schema—missing keys, wrong types, non-JSON output.

Sensitivity to phrasing: small changes in prompt language change outputs materially.

These are not rare. They are fundamental properties of this kind of tool. That’s why documentation and validation are non-negotiable.

---

## Slide 15 — Evaluation metrics

Extraction quality can be assessed with precision and recall, field-level accuracy, and agreement with human labels.

Evaluation depends on task goals.

This is an important nuance.

If your task is event detection, you might care about recall—capturing as many true events as possible.

If your task is building a high-precision dataset for causal inference, you might care more about precision—minimizing false positives.

Field-level accuracy matters because some fields are easier than others. The model might get dates right but actors wrong, or vice versa.

Agreement with human labels matters because ultimately you need a reference point. The model doesn’t define truth; the measurement framework does.

---

## Slide 16 — LLMs are not neutral

LLM outputs reflect training data biases, prompt framing, and model defaults.

Human oversight is essential.

This slide is about epistemic humility.

These models are trained on large corpora that reflect social biases and coverage patterns. They also reflect prompt framing: what you ask shapes what you get. And defaults matter: different models behave differently even with the same prompt.

So the right posture is: treat LLM outputs as model-generated measurements with known limitations—not as ground truth.

And bring back the course theme: the pipeline must be auditable, and limitations must be documented.

---

## Slide 17 — Documentation requirements

Every LLM pipeline should record model and version, prompt text, schema definition, validation procedure, and known limitations.

Transparency enables reuse.

This is essentially the reproducibility slide for LLM pipelines.

If you don’t record the model version, you can’t reproduce results later. If you don’t record the prompt, you can’t interpret what was asked. If you don’t record the schema, you can’t validate or compare outputs. If you don’t record the validation procedure, you can’t justify trust in the data.

So treat prompts and schemas as first-class artifacts. Put them in the repo, version them, and cite them.

---

## Slide 18 — What we emphasize in practice

Let me summarize the four key practices.

Use LLMs as assistants. Don’t outsource scientific responsibility.

Enforce structure aggressively. Schemas and output constraints are essential.

Validate with humans. Audits and review are not optional.

Document failure modes. Your dataset is only as credible as your disclosure of its limitations.

If you do these four things, LLM extraction becomes a defensible measurement procedure rather than a black box.

---

## Slide 19 — Discussion

Let me close with three questions.

Where do LLMs help most? Often it’s in accelerating labeling, extracting structured fields, and handling heterogeneity in text.

When do they fail quietly? Often when the text is ambiguous, when categories are underspecified, or when the model fills in plausible details that aren’t supported.

And how much validation is enough? There isn’t a universal answer. It depends on the stakes, the use case, and the tolerance for error. But the principle is: validation must be sufficient to prevent your downstream analysis from being dominated by systematic extraction error.

**[pause]**
