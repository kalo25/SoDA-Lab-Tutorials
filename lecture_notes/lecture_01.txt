# Lecture 1 script: Big Social Data — Approaches and Issues

## Slide 1 — Title

Alright—welcome. In this course we’re going to treat “big social data” as something more than a set of tools. The core idea is that modern social science increasingly depends on data that are large, messy, heterogeneous, and often produced by systems that were never built for research.

So this first lecture is going to set the course frame: what we mean by “social data,” what kinds of problems we’ll repeatedly return to, and how the course is organized week to week.

**[pause]**

---

## Slide 2 — Framing the course

I want to start with a simple claim: the work of social data is not only analysis. It’s everything upstream of analysis.

This course examines how social data are created, collected, curated, linked, validated, and shared—at scale, and under real research constraints.

Let me make that concrete. Suppose you see a paper with a model and results you like. The reliability of those results depends on things like: where the data came from, what got measured versus what got left out, how missingness was handled, how records were linked, whether units were duplicated, whether labels were consistent, and whether the whole pipeline can be rerun.

So in this class we’re going to push attention upstream. We’ll still talk about modeling, but we’ll treat the data pipeline as the object of methodological scrutiny.

**[pause]**

One more thing: “at scale” doesn’t just mean “bigger.” It means you start running into failure modes that don’t show up in small projects—things break silently, defaults matter, and quality problems can become systematic rather than idiosyncratic.

---

## Slide 3 — What counts as social data

What counts as social data?

In this course, “social data” includes information about—or arising from—human interaction, institutions and organizations, digital platforms, text, networks, spatial traces, and sensors.

So yes: surveys and administrative records still count. But we’ll also work with data that come from web pages, APIs, digitized documents, communications, and relational structures like co-sponsorship networks or online interaction networks.

A big reason this matters is that these modalities come with different assumptions. Text data force you to decide what counts as a document and how meaning is represented. Network data force you to decide what counts as a tie and what the unit of analysis is. Spatial data force you to care about coordinate systems, aggregation boundaries, and remote sensing measurement error.

And a lot of modern social data are produced by platforms or systems that are optimizing something other than scientific validity—engagement, moderation, compliance, ad revenue, logistical efficiency. That doesn’t make them unusable. It means we have to study them as artifacts of the systems that produced them.

**[pause]**

---

## Slide 4 — Data as a research object

Here’s the core premise of the course.

First: data are constructed, not found. Even when data feel “out there,” they only become analyzable after a chain of choices—what to scrape, what to store, how to parse, how to define entities, how to code ambiguity, which records to exclude.

Second: measurement choices are theoretical choices. If you’re studying “polarization,” your operationalization embeds a theory about what polarization is. If you’re studying “violence,” your event definition embeds a theory about what counts as violence. If you’re studying “influence,” your network construction embeds a theory about what influence looks like.

Third: collection decisions shape downstream inference. If coverage is uneven, you don’t just have noise—you may have structural bias. If missingness is correlated with the outcome, you don’t just lose power—you can change the sign of effects. If linkage errors concentrate in certain subpopulations, you build inequities into the dataset.

So when I say “data pipelines are part of the substantive argument,” I mean: if your pipeline builds the object you study, then your pipeline is a claim about the world. And it should be open to critique.

**[pause]**

---

## Slide 5 — How the course is organized

Let me explain how we’ll run each class session.

Every session follows a consistent structure built around real research practice. The goal is to connect tools, judgment, and substantive inference—rather than treating methods in isolation.

That’s intentional. In a lot of methods sequences, you learn a technique, and only later see an application. Here we’ll reverse that: you’ll see real workflows, then we’ll learn the tools, then we’ll evaluate an application.

There’s also a deeper reason: a lot of the hardest problems in big social data are not “how do I run this function?” They’re “what decision did I just make without noticing?” The structure of the course is designed to force those decisions into the open.

---

## Slide 6 — Component 1: Research workflow talk

Each session begins with a research talk from an outside speaker.

The focus is not just results. It’s data generation and workflow: what their unit of analysis is, how they constructed the dataset, what broke, what tradeoffs they made, what they would do differently, and where the inferential vulnerabilities are.

I want you to listen for a few recurring questions during these talks:

* What is the pipeline? Not the model—what is the pipeline?
* Where are the fragile points: scraping, linkage, labeling, geocoding, validation?
* What is the biggest validity threat, and how did they address it?
* What’s the “unknown unknown”: what might be missing entirely?

If you learn to hear research talks in that way, you’ll start to see that “data work” is not a pre-processing step. It’s a major part of the scientific contribution.

---

## Slide 7 — Component 2: Coding lab

The core of each session is the coding lab.

The important points are: the demo code is instructor-prepared, but a student lab lead runs the walkthrough. We’ll do live execution and debugging. And we’ll emphasize rerunnable pipelines.

That last phrase—rerunnable pipelines—is the key. The goal is not to get something to work once on your laptop. The goal is to build workflows that can be rerun safely and explained clearly.

In practical terms, that means you’ll see patterns like: saving raw data separately, logging progress, making code idempotent—meaning you can run it twice without corrupting outputs—capturing environments, and documenting assumptions.

And importantly: we’re treating code as a shared research artifact. Code is not just a personal tool; it’s a way of communicating what you did. If a reader can’t understand how your data were generated, they can’t evaluate your claims.

**[optional shortener]** If you need to keep this lecture shorter, you can stop here and move directly to the next slide; the rest of this slide’s message repeats later.

---

## Slide 8 — Component 3: Application paper discussion

The third component is a discussion of an application paper that uses the week’s tools.

The goal is critical evaluation, not replication. We’re going to use the paper to practice asking: how does the data pipeline affect the substantive claim?

So in discussion, we’ll focus on things like:

* What are the units and where do they come from?
* What are the key measurement choices?
* What are the validity threats and how are they handled?
* What would we need to know to trust the result?

You’ll notice that this course repeatedly returns to a single meta-skill: the ability to evaluate whether a data product is trustworthy. That skill transfers across modalities—web data, surveys, platforms, text, networks, spatial data, and machine-assisted extraction.

---

## Slide 9 — Research pipelines we emphasize

Now I want to name the pipeline stages we’ll keep emphasizing: acquisition, storage, cleaning, linkage, documentation, and validation—with explicit provenance at each step.

Let me define provenance the way we’ll use it: provenance is the story of where the data came from, when and how it was collected, what transformations happened, and what limitations are known.

You can think of provenance as the difference between “here is a CSV” and “here is a dataset you can defend.”

And notice the arc here: acquisition and storage are not glamorous, but they determine whether the rest of your work is stable. Cleaning and linkage determine what your units actually are. Documentation and validation determine whether anyone—including you in six months—can trust the output.

In a sense, the pipeline is the discipline. It forces you to separate raw sources from processed products, and it forces you to make the transformations explicit.

---

## Slide 10 — Tooling is methodology

This slide is one of the most important ideas in the course: tooling is methodology.

Tools shape what is easy, hard, or invisible. Programming languages, version control, databases, containers, automation—these don’t just make you faster. They make certain kinds of research possible and other kinds less likely.

For example:

* If you don’t use version control, you will not have a transparent history of analytic choices.
* If you don’t use a database when you should, you will end up with fragile joins in memory and silent duplicates.
* If you don’t capture environments, you’ll get “it worked on my machine” failures—especially when collaborators use different operating systems.
* If you automate without validation, you can scale errors.

So we’ll repeatedly treat the tools themselves as part of the scientific apparatus. Not because you need the newest thing, but because you need workflows that are inspectable and repeatable.

**[pause]**

---

## Slide 11 — Validation and failure modes

Modern data work requires validation as a first-class task.

By validation, I mean sanity checks, audits and spot checks, inter-coder reliability where relevant, and monitoring drift and bias.

This is where big social data can be counterintuitive: as you scale up, you can’t manually inspect everything, so you need systematic checks. And when your data are produced by platforms or complex processes, drift is normal. The same keyword query might retrieve different kinds of content over time. Platform rules change. User populations change. Reporting practices change.

So validation isn’t something you do once at the end. It’s a practice you embed into the pipeline.

If you learn one habit from this course, I’d want it to be this: whenever you produce an output dataset, you should have at least a few checks that would catch obvious problems—unexpected missingness, impossible values, duplication, shifts in distribution, structural breaks over time.

---

## Slide 12 — Human-in-the-loop workflows

This course also includes human-in-the-loop workflows, especially around LLM-assisted extraction and labeling.

Here’s the principle: machine assistance does not remove judgment. It changes what judgment looks like.

We’ll use LLMs for tasks like structured extraction and labeling. But humans remain responsible for validation and adjudication, and for documenting uncertainty and failure modes.

So the focus is augmentation, not replacement.

And we’ll be very explicit about the risks: models can be confident and wrong, failure can be systematic, and you can accidentally build a pipeline that looks precise but is not trustworthy.

The human-in-the-loop framing keeps the scientific responsibility where it belongs: on the researcher.

---

## Slide 13 — What you should be able to do

By the end of the course, you should be able to do four things.

First: design a defensible data pipeline. Not just a script, but a pipeline that can be explained and rerun.

Second: evaluate data quality and bias. That means understanding where bias enters—coverage, selection, measurement, drift—and being able to argue about its consequences.

Third: reproduce and audit workflows. That includes running someone else’s pipeline, identifying fragile points, and checking whether conclusions depend on hidden choices.

Fourth: communicate limitations clearly. A major part of credible research is being clear about what your data do not capture and what assumptions your inference requires.

These are foundational skills. They apply to your own work, but they also apply when you read papers, review papers, and assess evidence in policy contexts.

---

## Slide 14 — Discussion

Let me close with three questions.

First: where do you think bias enters most often? Is it at collection? At labeling? At linkage? At interpretation? In my experience, bias is often introduced early—at coverage and selection—and then becomes very hard to fix later.

Second: which steps of a pipeline feel least transparent? For most people, it’s the early steps—scraping decisions, parsing rules, linkage thresholds, exclusion criteria—because those are the steps that rarely show up in the published paper.

Third: what makes a data product trustworthy? If you had to evaluate a dataset you didn’t build, what would you need to see? A README? Provenance notes? Validation checks? A rerunnable script? Versioned environments?

As you watch the pre-class videos and do the labs this semester, keep returning to that question: what would it take for someone else to trust this data product?

**[pause]**

Alright. That’s the framing lecture. In the next lecture we’ll get practical and start working through concrete acquisition patterns and how web data become structured data.

