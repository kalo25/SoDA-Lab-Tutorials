# Lecture 7 script: Text as Data Pipelines

## Slide 1 — Title

In this lecture we’re going to talk about text as data, but I want to be clear up front about the framing. Text analysis is not a single method. It is a pipeline.

The key theme today is that the biggest threats to validity in text analysis usually happen upstream: in corpus construction, in preprocessing, and in representation. Models matter, but the pipeline decisions that feed the model often matter more.

So I’ll walk through the pipeline end-to-end: collection, cleaning, representation, modeling, and validation. And as we go, I’ll emphasize what each step assumes, what it throws away, and how it can quietly bias the resulting measure.

**[pause]**

---

## Slide 2 — Why text is data

Text is one of the most common forms of social data.

Large volumes of social information appear as news articles, speeches, statements, social media posts, reports, and documents. If you want to study political communication, institutional behavior, ideologies, blame, legitimacy, threat framing—text is often the primary observable trace.

But text requires structured transformation before analysis. Text is not naturally in rows and columns. It is unstructured content, and our job is to map it into representations that support measurement and inference.

So the core message here is: text is data, but only after we construct a measurement pipeline.

---

## Slide 3 — Text as a pipeline

This is the organizing slide.

Text analysis is a pipeline:

1. collection,
2. cleaning,
3. representation,
4. modeling,
5. validation.

Each stage shapes inference.

That’s not a slogan—it’s literal. If you change what documents you include, you change the population of texts and the meaning of any derived measure. If you change preprocessing, you change what counts as the same word, what gets removed, and what remains. If you change representation, you change what “similarity” means. If you change the model, you change how patterns are summarized. And if you don’t validate, you can easily produce measures that look plausible but don’t mean what you think they mean.

So throughout the lecture, keep asking: what is this step doing to meaning? What does it preserve, and what does it discard?

---

## Slide 4 — Raw text is not analysis-ready

Raw text contains noise: formatting and markup, boilerplate and duplicates, irrelevant tokens.

And the key line here is: preprocessing decisions are theoretical decisions.

This is a point people often underestimate. Preprocessing feels like housekeeping. But it’s actually the stage where you decide what counts as signal.

For example: if you remove stopwords, you’re assuming function words don’t carry meaning for your task. Often that’s fine, but not always. If you lowercase, you assume capitalization isn’t meaningful. If you stem or lemmatize, you collapse variants and assume they’re the same concept. If you remove punctuation, you may lose negation cues or emphasis. If you strip numbers, you might remove references to years, casualty counts, or policy codes.

So preprocessing is a measurement instrument. You should treat it with the same seriousness you’d treat survey question wording.

**[pause]**

---

## Slide 5 — Tokenization (pseudocode)

Tokenization is the step where we break text into units.

Those units can be words, subwords, or characters. And the choice matters.

The pseudocode here is a minimal pipeline: load a corpus, tokenize each document, normalize tokens, remove stopwords, and store the tokenized result.

The key concept is: token choice affects meaning and scale.

Word tokenization is transparent and intuitive, but it struggles with rare words, morphology, and spelling variation. Subword tokenization—common in transformer models—handles unknown words more gracefully and lets the model represent word parts. Character tokenization can handle misspellings and morphology but changes the unit of meaning even more.

So tokenization is not just a mechanical step. It’s the first major representational choice in the pipeline.

---

## Slide 6 — Bag-of-Words construction

Bag-of-words is the classic baseline representation.

The pseudocode shows the logic: build a vocabulary from tokens, initialize a document-term matrix, and then for each document count occurrences of tokens.

The important conceptual point is right at the bottom: bag-of-words prioritizes transparency over semantics.

It’s “bag-of-words” because it largely ignores word order. That’s a feature and a limitation.

The feature is interpretability: you can inspect what words drive a model. You can audit the vocabulary. You can see what is being counted.

The limitation is that a lot of meaning is in context and word order. “Not good” and “good” can look similar if order is discarded. Irony and sarcasm vanish. Negation is hard. Multiword expressions are broken apart unless you explicitly include them.

So bag-of-words is a useful baseline and often a defensible measure—especially if your question is about what words are used—but you should be aware of what it throws away.

---

## Slide 7 — TF-IDF weighting

TF-IDF is a refinement on bag-of-words.

The pseudocode shows the idea: compute document frequency for each term, compute inverse document frequency, and then weight the document-term matrix by IDF.

The intuition is that TF-IDF downweights ubiquitous terms. Words that appear in almost every document carry less discriminating information. Words that are rare carry more.

This is useful for tasks like information retrieval and similarity search, and it can improve clustering and classification.

But again, this is a measurement choice: TF-IDF implicitly values distinctiveness over commonality. That’s often what you want, but it can also downweight genuinely important shared vocabulary.

So TF-IDF is still “counting,” but with a principled reweighting that changes what your representation emphasizes.

---

## Slide 8 — Embeddings (conceptual)

Embeddings are a different kind of representation.

Instead of representing a document as counts over a vocabulary, embeddings map text to vectors in a continuous space. Similarity becomes geometric: texts with similar meaning are closer in that space.

The pseudocode here is intentionally minimal: load a pretrained embedding model, encode each document, and store the vectors.

The key line is: embeddings shift interpretation from words to geometry.

That’s a big conceptual move. You gain semantic smoothness—synonyms can be close even if they’re different words—but you lose some transparency. You’re no longer saying “this word count increased”; you’re saying “this vector moved in space.”

So you should think of embeddings as a tradeoff: they often improve performance and capture semantics, but you need stronger validation to ensure the representation aligns with your substantive concept.

---

## Slide 9 — Topic models (conceptual)

Topic models are a way to summarize large corpora by discovering latent themes.

The pseudocode is generic: initialize K topics, iterate updates until convergence, and return topic distributions and document assignments.

The line I want to emphasize is: topics summarize patterns; they are not ground truth.

Topic models don’t “discover” the real structure of politics. They provide a descriptive decomposition of co-occurrence patterns given your preprocessing and model assumptions.

And topic models are especially sensitive to preprocessing. If you change the vocabulary, remove or keep certain words, change how you handle rare terms, or change document segmentation, topics can shift.

So topic models can be useful—especially for exploration and summarization—but they demand careful interpretation and stability checks.

---

## Slide 10 — Transformer workflow (lightweight)

Transformers are currently the dominant paradigm for many text tasks.

But I want to keep this lightweight and pipeline-oriented.

The pseudocode shows the core steps:

* tokenize with a model-specific tokenizer,
* encode context-aware representations,
* pool the representations—for example using a special classification token,
* and then optionally fine-tune or classify.

The key sentence is: transformers model context explicitly.

That’s the conceptual improvement over bag-of-words. Word meaning is conditioned on surrounding words. “Bank” in a finance context and “bank” in a river context can be represented differently.

But the pipeline lesson remains: transformers don’t eliminate preprocessing or corpus decisions. They change what representation is, and they raise the stakes on validation because the representations are more complex and less interpretable.

So, again: think of transformers as pipelines, not magic boxes.

---

## Slide 11 — Validation and stability checks

This is the slide where I want you to slow down.

Text models can fail silently. That means you can get outputs that look plausible but reflect artifacts of preprocessing, sampling, or model defaults.

So we validate and check stability.

The pseudocode gives two simple patterns:

* sensitivity to preprocessing variants: run the pipeline under alternative cleaning choices and compare results;
* stability across random seeds: rerun with different seeds and see if conclusions change.

These are basic habits, but they make an enormous difference. If your results depend strongly on whether you removed numbers or whether you used stemming, that’s not necessarily fatal—but it must be reported, and it should shape how strongly you claim substantive conclusions.

So: treat stability checks as part of the method, not a luxury.

---

## Slide 12 — Error and bias propagation

Bias can enter through corpus selection, tokenization choices, pretrained models, and labeling procedures.

And the final line matters: upstream choices dominate downstream results.

Let’s unpack that.

Corpus selection: if you build a corpus from a platform, the platform population and algorithms shape what texts exist. If you build from newspapers, journalistic practices shape what’s reported. If you build from speeches, institutional incentives shape what is said.

Tokenization and preprocessing: you decide what is removed, what is normalized, and what counts as the same token.

Pretrained models: embeddings and transformers inherit biases from training data and the broader text distribution they were trained on.

Labeling: if you use humans or crowdsourcing to label text, you import coder interpretation and category ambiguity into the data.

So text pipelines are not neutral. They embed social and institutional processes, and that’s exactly why we have to treat them as measurement.

---

## Slide 13 — Documentation requirements

Every text pipeline should record the corpus source, preprocessing steps, representation choice, model parameters, and validation checks.

This is the reproducibility connection.

If someone else can’t see what preprocessing you did, they can’t reproduce or evaluate your measure. If they can’t see what model and parameters you used, they can’t interpret differences. If they can’t see validation checks, they can’t assess whether the measure is stable.

So: auditable pipelines are reproducible pipelines. That’s the standard we’re aiming for across the course.

---

## Slide 14 — What we emphasize in practice

Let me summarize what we emphasize.

Pipelines over single models: don’t get fixated on “the model.” The pipeline defines the data product.

Simplicity before complexity: start with bag-of-words or simple embeddings, validate, then move to transformers if you need them.

Validation over accuracy: accuracy is not enough if the measure is unstable or biased.

Transparency in preprocessing: preprocessing is where meaning is constructed and lost. Make it visible.

If you internalize these four principles, you’ll avoid the most common failure in text as data: producing a measure that looks quantitative but is conceptually ungrounded.

---

## Slide 15 — Discussion

Let me close with a few questions.

Where does meaning get lost in text pipelines? Is it in tokenization, removal, representation, or modeling?

Which pipeline decisions matter most for your own research questions? If you care about framing, you might care about context. If you care about agendas, you might care about topic stability. If you care about ideology, you might care about embedding geometry.

And how should uncertainty be communicated? If a text-derived measure is sensitive to preprocessing, what does it mean to report that? Should you show robustness across variants? Should you present a range rather than a point estimate?

Keep these questions in mind as you do the lab work. The lab will give you tools, but the real goal is learning to think about text as a measurement pipeline.

**[pause]**
