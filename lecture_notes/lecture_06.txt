# Lecture 6 script: Record Linkage and Entity Resolution

## Slide 1 — Title

In this lecture we’re focusing on record linkage and entity resolution—how to merge messy records and decide when two rows in different datasets refer to the same real-world entity.

This is one of the most important topics in the course because linkage is how we construct units of analysis. In many projects, what looks like “data cleaning” is actually the stage where you decide who exists in the dataset and how you count them.

So the framing today is simple: entity resolution is measurement. It’s not clerical cleanup. It’s an inferentially consequential step that can introduce systematic error if you treat it casually.

**[pause]**

---

## Slide 2 — Why record linkage matters

Many social datasets refer to the same entities, use inconsistent identifiers, and were never designed to be merged.

This shows up everywhere: people’s names with different spellings, organizations that change names, locations that are represented at different granularities, administrative records with partial fields, or platforms that use different user IDs across products.

When you try to combine those datasets, you face the question: what counts as the “same” entity?

That’s what linkage answers. And because linkage defines the unit of analysis, linkage errors can change your substantive conclusions.

A useful way to say this: before you can estimate a model, you have to define what the observations actually are. Linkage is often the hidden step where that definition happens.

---

## Slide 3 — Entity resolution as measurement

This slide makes the key claim explicit.

Entity resolution determines: who exists, which records correspond, and how units persist over time.

Think about each of those.

“Who exists” means: do we treat two name variants as one person or two people? Do we treat a renamed organization as continuous or new? Do we treat multiple address records as the same household? Those are identity decisions.

“Which records correspond” means: are we willing to accept imperfect matches, and under what conditions?

“How units persist” means: if identifiers change over time, how do we maintain continuity?

So the key lesson: this is not clerical cleanup. It is measurement, and measurement decisions are theory-laden.

**[pause]**

---

## Slide 4 — Deterministic vs probabilistic logic

Now we introduce the two broad families of linkage logic.

Deterministic rules are the simplest: if name matches and date of birth matches, link the records; otherwise, don’t.

This is attractive because it’s transparent and easy to implement. But it often fails in practice because exact matches are rare and fields are missing or noisy.

Probabilistic logic treats linkage as a likelihood problem: the match probability is a function of multiple similarity signals—name similarity, date similarity, location similarity—and then you link if the probability exceeds a threshold.

The final line matters: thresholds encode tradeoffs.

Any linkage system balances false positives and false negatives. A strict threshold reduces false matches but increases missed matches. A lenient threshold increases matches but increases errors.

So even at this high level, linkage is about tradeoffs, not perfect solutions.

---

## Slide 5 — Blocking strategy

Blocking is how we make linkage computationally feasible.

Naively, comparing every record in dataset A to every record in dataset B is quadratic. That explodes fast.

Blocking introduces an assumption: only compare pairs that share some crude characteristic—like first letter of last name and birth year.

The pseudocode here shows the basic idea: assign records to blocks using a blocking key, then only compare within blocks to generate candidate pairs.

The point on the slide is exactly right: blocking reduces complexity but risks missed matches.

So blocking is not just an engineering step. It’s another measurement decision. If your blocking is too aggressive, you never even consider true matches. If it’s too loose, you lose the computational benefit.

**[pause]**

A practical habit: treat blocking as a tunable design choice, and assess sensitivity to alternative blocking rules.

---

## Slide 6 — Feature construction for matching

Once you have candidate pairs, you need to create features that quantify similarity.

This slide shows the idea: compute a set of comparison features like name similarity, date similarity, and geographic distance.

The key point is the last line: distances are evidence, not decisions.

A string similarity score isn’t itself a match. It’s a signal. The model—explicitly or implicitly—combines those signals to produce a match judgment.

This is why people get into trouble when they treat a single similarity metric as truth. Similarity depends on context: “USA” and “United States of America” are very similar semantically but not character-by-character. “John Smith” is an exact match but may refer to many distinct individuals.

So the general approach is: construct multiple features, understand their failure modes, and combine them in a rule or model.

---

## Slide 7 — Fellegi–Sunter logic

This is the classical probabilistic linkage framework.

You compute a likelihood ratio: the probability of seeing the feature pattern if the pair is a true match divided by the probability of seeing it if it’s a non-match.

Then you apply a three-way decision rule:

* if the evidence is strong, classify as match
* if the evidence is weak, classify as non-match
* if it’s ambiguous, send to clerical review

The key line is: uncertainty is explicit.

That’s one reason this framework remains useful conceptually. It doesn’t pretend linkage is always binary. It introduces a middle region of uncertainty.

In practice, “clerical review” might mean a human adjudicator, or a secondary model, or a targeted audit. But the conceptual point stands: good linkage workflows explicitly handle ambiguity.

---

## Slide 8 — EM-style estimation

Now we move from logic to estimation.

The slide describes a generic EM-style approach: initialize parameters, then iterate between estimating match probabilities for each candidate pair and updating model parameters based on those probabilities.

The big lesson is the last line: linkage models are estimated, not assumed.

You don’t have to hard-code the relative importance of name similarity versus date similarity. You can estimate how predictive each feature is for true matches—especially if you have labeled training pairs or you use assumptions about match/non-match distributions.

Even if you don’t implement EM directly, you should carry the intuition: probabilistic linkage is a statistical model, and it has parameters and assumptions that can be evaluated.

---

## Slide 9 — Many-to-one resolution

This slide highlights a practical reality: conflicts.

Sometimes one record in dataset A matches multiple records in dataset B with high probability. Or vice versa. You need a resolution rule.

The pseudocode shows a simple rule: for each entity, keep the candidate with the highest match probability and discard others.

The key warning is important: resolution rules affect downstream counts.

For example, if you collapse too aggressively, you undercount entities. If you allow too many duplicates, you overcount. If you apply asymmetric rules, you can bias one side of a merge.

So resolution is not just cleanup; it’s a counting rule. It influences estimates of prevalence, totals, network degree counts—many downstream statistics.

A good habit is to report how many conflicts occurred and how you resolved them.

---

## Slide 10 — Evaluation with labeled data

When you have labeled pairs, you can evaluate linkage quality using precision and recall.

Precision asks: of the links you predicted, how many are truly correct?

Recall asks: of the true matches that exist, how many did you find?

And the line “no single metric dominates” matters because the right tradeoff depends on the research context.

In some settings, false positives are disastrous—linking two different people as one. In others, false negatives are more costly—missing a true match and splitting an entity into two.

So evaluation is not just reporting numbers. It’s choosing metrics that match your inferential priorities.

---

## Slide 11 — Sensitivity analysis

Because linkage involves thresholds and design choices, sensitivity analysis is essential.

This slide shows the basic idea: vary thresholds, rerun linkage, compare outcomes.

The key line is: robustness matters more than point estimates.

If your downstream results change dramatically when you move the threshold from 0.85 to 0.90, that means your inference depends strongly on a somewhat arbitrary decision.

That doesn’t necessarily invalidate the work, but it does require transparency: you should report sensitivity, potentially incorporate linkage uncertainty, and avoid presenting a single linkage output as ground truth.

---

## Slide 12 — Error propagation

This slide is a major conceptual point.

If you fit a downstream model using linked data, linkage uncertainty propagates into the model.

When people ignore linkage error, they often understate uncertainty and sometimes induce bias.

Linkage errors are often correlated: certain names, certain groups, or certain regions are more error-prone. That can create systematic distortion in estimates of group differences or geographic patterns.

So one of the key practices of high-quality linkage work is to treat linkage as an uncertainty-producing step, not a deterministic preprocessing step.

There are different ways to handle this in practice—multiple imputation style approaches, probabilistic weighting, sensitivity bounds—but the conceptual commitment is the same: don’t pretend linkage is perfectly known if it isn’t.

---

## Slide 13 — Documentation requirements

Every linkage pipeline should record blocking rules, similarity metrics, thresholds, evaluation results, and known failure modes.

This is a direct extension of the course’s reproducibility theme. Linkage is full of tunable parameters. If those aren’t documented, the dataset cannot be audited or reproduced.

Also, “known failure modes” is crucial. For example: certain transliteration patterns, certain name structures, or certain missingness patterns may break the model. If you identify those, they should be recorded so readers know what the dataset is less reliable for.

---

## Slide 14 — What we emphasize in practice

Let me summarize the four course principles for linkage.

Treat linkage as measurement. Identity decisions create the data you analyze.

Preserve uncertainty. Don’t collapse uncertainty away without accounting for it.

Evaluate tradeoffs explicitly. Your design choices should be justified and tested.

Document all design choices. A linkage pipeline without documentation is not a scientific artifact.

If students internalize these, they will avoid the most common linkage failure: treating a merge as a purely mechanical step.

---

## Slide 15 — Discussion

Let me close with three questions.

Where are false positives most costly? Think about situations where merging distinct entities would distort conclusions—like estimating the prevalence of a behavior, or attributing actions to the wrong actor.

When are false negatives more dangerous? Think about situations where splitting an entity into multiple records would fragment networks, undercount participation, or reduce measured persistence.

And how should linkage uncertainty be reported? What would it mean to communicate a dataset as probabilistic rather than definitive? In many contexts, it means reporting sensitivity and uncertainty rather than claiming a single cleaned dataset is the truth.

**[pause]**

