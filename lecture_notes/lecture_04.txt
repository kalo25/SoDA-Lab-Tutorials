# Lecture 4 script: Surveys, Platforms, and Crowdsourcing

## Slide 1 — Title

In this lecture we shift from web and API collection into a different—but equally central—data source: surveys, platforms, and crowdsourcing.

The key theme today is that survey data are not just “responses.” They are the output of a measurement process. And that process increasingly happens on platforms—meaning the platform shapes who answers, how they answer, and what the data quality looks like.

So I’ll focus on three things: first, why surveys remain important; second, how platforms change survey research; and third, what a modern survey pipeline looks like in practice—exports, codebooks, recoding, quality checks, and missingness.

**[pause]**

---

## Slide 2 — Why surveys still matter

Surveys remain central to social science because they let us measure attitudes, beliefs, and preferences—especially latent concepts that don’t show up directly in behavior.

A survey is one of the few instruments we have that can ask: what do you think, what do you want, what do you believe, how do you interpret something.

But the caution on this slide is important: modern surveys are rarely simple random samples.

Even when a survey is professionally designed, there are usually compromises—coverage constraints, nonresponse, mode effects, and the realities of recruiting respondents in modern environments.

So the survey method is still powerful, but it comes with an obligation to think clearly about who is being measured, what is being measured, and how measurement error enters.

---

## Slide 3 — The shift to platforms

A major shift in the last decade is that much contemporary survey data are collected through platforms: online labor markets, survey platforms, panel providers, and hybrid recruitment strategies.

Platforms reshape who answers and how.

This matters because “platform” is not a neutral channel. Platforms implement recruitment, compensation, attention checks, interface design, and sometimes a whole ecosystem of respondent norms.

So even if two surveys ask the same questions, administering them on different platforms can produce different response distributions.

A key takeaway: when you use platform data, platform choice becomes part of the research design.

---

## Slide 4 — Data as a measurement process

This slide is the conceptual core.

Survey data reflect question wording, response options, ordering and framing, and the mode of administration.

Those aren’t minor details—they are the measurement instrument.

So measurement decisions precede analysis.

If you ask a question in a different way, you change what is being measured. If you change response options, you change what respondents can express. If you change ordering, you can prime respondents. If you change the mode—online vs phone vs in-person—you can change social desirability bias, attention patterns, and dropout.

So the habit I want you to build is: before you analyze a survey dataset, reconstruct the measurement process. Treat the questionnaire as part of the data.

---

## Slide 5 — From instrument to dataset

In practice, surveys arrive as exported files—CSV, TSV, SPSS—plus codebooks or questionnaires, metadata files, and platform-generated diagnostics.

The workflow implication is straightforward: understanding structure comes first.

A dataset called “survey_export.csv” is not self-explanatory. Variable names are often cryptic. Missing values may be encoded as special numbers. Skip patterns create structured missingness. And platforms generate diagnostic fields that can be easy to ignore but often matter.

So before “analysis,” you do data archaeology: you look at the codebook, you inspect the structure, and you figure out what the dataset actually means.

---

## Slide 6 — Reading survey exports (Python)

This slide is intentionally simple: read the file and inspect.

The main point is the sentence at the bottom: the raw export is not analysis-ready data.

Reading the file is trivial. Understanding it is the work.

So the first step is always: load, inspect types, inspect missingness, inspect frequency tables for key variables, and then compare those to what the codebook says.

---

## Slide 7 — Reading survey exports (R)

Same in R: load, inspect.

Again, the method is not language-specific: always inspect before cleaning.

If students learn only one pipeline habit for survey data, it should be: never recode a variable until you’ve verified what its values mean and how missingness is encoded.

---

## Slide 8 — Codebooks are data

This slide is short but crucial.

Codebooks specify variable meanings, scales and categories, missing value codes, and skip logic.

Ignoring codebooks leads to silent errors.

And “silent error” is the nightmare scenario: you recode something incorrectly, your model runs fine, your results look plausible, but you have effectively measured something else.

So in a pipeline, the codebook isn’t optional documentation—it’s part of the dataset. It’s the decoder ring.

---

## Slide 9 — Labeling and recoding

Cleaning survey data often requires relabeling categories, harmonizing scales, converting strings to factors, and explicit missing-value handling.

The emphasis is on explicitness.

The biggest danger in survey cleaning is implicit recoding—letting defaults decide what missing means, letting software guess types, or letting labels be interpreted inconsistently.

So your goal is to translate “raw export” into “analysis dataset” with transformations that are transparent and reproducible.

---

## Slide 10 — Recoding example (Python)

This example is intentionally simple: mapping numeric codes to labeled categories.

The broader point is that recoding is part of measurement. When you map codes to labels, you’re defining the categories that exist in your analysis.

And once you start combining categories—say, merging multiple responses into a single bin—you are building a theoretical simplification. That can be defensible, but it must be documented.

So: recode explicitly, and record what you did and why.

---

## Slide 11 — Recoding example (R)

Same idea, same warning.

You want transformations that are transparent, rerunnable, and easy to audit.

And notice something we’ll keep returning to: when you build a pipeline, your code becomes your methods section. The code is the specification of your measurement procedure.

---

## Slide 12 — Crowdsourcing and human subjects

Crowdsourced data introduce additional complications.

Crowdsourced respondents are heterogeneous. Attention and effort vary. Strategic responding can occur—especially when respondents learn the incentive structure.

So quality is uneven and must be assessed.

This is where students often get tripped up: they assume “survey equals controlled measurement.” But crowdsourced surveys are often closer to “semi-controlled measurement,” because the environment is less supervised.

That doesn’t mean you can’t use them. It means you should anticipate quality variation and build checks into the pipeline.

---

## Slide 13 — Quality checks are essential

Quality checks are not just niceties. They are measurement decisions.

Common checks include completion time thresholds, attention checks, straight-lining detection, and response consistency.

Each of these reflects a theory about what low-quality data look like.

And importantly, these checks can introduce their own bias. For example, a completion time threshold can disproportionately exclude certain populations or device types. Attention checks can exclude respondents who interpret a question differently or have language barriers. So quality checks must be justified and documented.

We’ll treat them as part of the measurement procedure, not as post hoc cleanup.

---

## Slide 14 — Simple quality check (Python)

This is the simplest version: filter out surveys completed too fast.

The point isn’t the exact threshold. The point is that you define a rule, you apply it transparently, and you document it.

In practice, you might do something more nuanced—like examining the distribution of completion times and setting a threshold based on the lower tail.

But the core idea remains: quality filtering should be explicit and reproducible.

---

## Slide 15 — Simple quality check (R)

Same concept.

And this is a broader pipeline lesson: every filter changes the dataset. Filters shape representativeness and can create selection effects. That’s why “document quality filters” appears later as a core practice.

---

## Slide 16 — Missingness is informative

This is one of the most important slides.

Missing data may reflect survey design, respondent fatigue, sensitivity of questions, and platform defaults. Missingness is rarely random.

So missingness is not just a statistical nuisance. It is evidence about the data-generating process.

If respondents systematically skip sensitive questions, that tells you something about the measurement instrument and the population. If fatigue increases missingness later in the survey, ordering matters. If platform defaults lead to missing fields, the interface matters.

So the pipeline lesson is: you should characterize missingness early. Look at missingness patterns across variables, across time, and across respondent types.

---

## Slide 17 — Platforms shape data

Platforms influence who participates, incentives and compensation, sampling frames, and longitudinal availability.

I want to underline: platform choice is a design choice.

Platforms shape the respondent pool, and they also shape the norms of responding. Some platforms develop a culture of “professional respondents.” Some cultivate particular demographics. Some differ in attention patterns, device usage, or attrition.

So you should treat platform selection the way you’d treat sampling frame selection. It’s part of what makes the data.

---

## Slide 18 — Documentation is non-negotiable

Every survey pipeline should record recruitment method, platform used, field dates, exclusion criteria, and known limitations.

The key point: survey data without documentation are not reusable.

Reusability is a serious scientific issue. If someone else can’t understand how the data were produced, they can’t evaluate the evidence. And if you can’t reconstruct how you produced your own dataset six months later, you can’t defend your own results.

So documentation is not optional. It’s part of the data product.

---

## Slide 19 — What we emphasize in practice

Let me summarize the practices we’ll emphasize.

First: treat surveys as constructed instruments. The questionnaire and platform are part of the method.

Second: inspect raw exports carefully. Do not assume labels are correct. Do not assume missingness is simple.

Third: make recoding explicit. Recoding is measurement. Record what you did.

Fourth: document quality filters. Filters shape who is included. They should be visible and defensible.

If you do those four things, your survey pipeline will be far more credible.

---

## Slide 20 — Discussion

Let me close with the discussion questions.

Where do surveys introduce bias? Is it in recruitment, nonresponse, platform selection, question wording, or measurement error?

How do platforms shape who responds? Which populations are overrepresented or underrepresented?

And which quality checks feel defensible? What rules would you use, and how would you justify them? What tradeoffs do those checks impose?

Keep these questions in mind for the lab. The lab will emphasize cleaning and codebooks, but the bigger goal is learning to treat survey workflows as part of methodology.

**[pause]**
